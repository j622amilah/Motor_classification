{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7dc75c7",
   "metadata": {},
   "source": [
    "# Run the pipeline classification by file OR with cells below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc21d6f",
   "metadata": {},
   "source": [
    "## By file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75cf030",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.insert(1, 'C:\\\\Users\\\\jamilah\\\\Documents\\\\Github_analysis_PROJECTS\\\\Motor_classification\\\\')\n",
    "\n",
    "from e_classification.main_script import *\n",
    "\n",
    "# ------------------------------\n",
    "# Run all classification combinations by a single file\n",
    "# ------------------------------\n",
    "main_script()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31cee4c",
   "metadata": {},
   "source": [
    "## By cells (for troubleshooting and it runs 4 to 6 times faster than the textfile with interpretor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "138787f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from scipy import signal\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, 'C:\\\\Users\\\\jamilah\\\\Documents\\\\Github_analysis_PROJECTS\\\\Motor_classification')\n",
    "from c_calculate_metrics.put_timeseries_trialdata_into_pandas import *\n",
    "\n",
    "# Personal python functions\n",
    "from subfunctions.make_a_properlist import *\n",
    "from subfunctions.numderiv import *\n",
    "from subfunctions.freqresp_functions import get_freqresp_mag_phase, select_fc, select_filter\n",
    "from subfunctions.freq_from_sig_timecounting import *\n",
    "from subfunctions.freq_from_sig_freqresp import *\n",
    "from subfunctions.my_dropna_python import *\n",
    "from subfunctions.scale_feature_data import *\n",
    "from subfunctions.normal_distribution_feature_data import *\n",
    "from subfunctions.findall import *\n",
    "from subfunctions.explode_without_colnames2 import *\n",
    "from subfunctions.scikit_functions_binaryclass import *\n",
    "from subfunctions.scikit_functions import *\n",
    "\n",
    "df_timeseries_exp = put_timeseries_trialdata_into_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341301fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('shape of df_timeseries_exp[rot] : ', df_timeseries_exp['rot'].shape)\n",
    "df_timeseries_exp['rot'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4eec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('shape of df_timeseries_exp[trans] : ', df_timeseries_exp['trans'].shape)\n",
    "df_timeseries_exp['trans'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "569aa04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(ax_val, ss_val):\n",
    "    \n",
    "    if ax_val == 'all' and ss_val == 'all':\n",
    "        # All the data\n",
    "        df_timeseries_exp[exp].head()\n",
    "        df = df_timeseries_exp[exp]\n",
    "    elif ax_val != 'all' and ss_val == 'all':\n",
    "        # Prediction for each axis\n",
    "        if ax_val == 'ax0':\n",
    "            ax_val_n = 0\n",
    "        elif ax_val == 'ax1':\n",
    "            ax_val_n = 1\n",
    "        elif ax_val == 'ax2':\n",
    "            ax_val_n = 2\n",
    "        df = df_timeseries_exp[exp][(df_timeseries_exp[exp].ax == ax_val_n)]\n",
    "    elif ax_val == 'all' and ss_val != 'all':\n",
    "        # Prediction per sup/sub\n",
    "        if ss_val == 'sup':  # sup\n",
    "            df = df_timeseries_exp[exp][(df_timeseries_exp[exp].ss > 0)]\n",
    "        elif ss_val == 'sub':  # sub\n",
    "            df = df_timeseries_exp[exp][(df_timeseries_exp[exp].ss < 0)]\n",
    "    elif ax_val != 'all' and ss_val != 'all':\n",
    "        # Prediction per axis and sup/sub\n",
    "        if ax_val == 'ax0':\n",
    "            ax_val_n = 0\n",
    "        elif ax_val == 'ax1':\n",
    "            ax_val_n = 1\n",
    "        elif ax_val == 'ax2':\n",
    "            ax_val_n = 2\n",
    "        \n",
    "        if ss_val == 'sup':  # sup\n",
    "            df = df_timeseries_exp[exp][(df_timeseries_exp[exp].ss > 0) & (df_timeseries_exp[exp].ax == ax_val_n)]\n",
    "        elif ss_val == 'sub':  # sub\n",
    "            df = df_timeseries_exp[exp][(df_timeseries_exp[exp].ss < 0) & (df_timeseries_exp[exp].ax == ax_val_n)]\n",
    "\n",
    "    print('Confirmation : exp=', exp, ', ax_val=', ax_val, ', ss_val=', ss_val)\n",
    "    \n",
    "    # Reset the index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90cadaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the joystick stimulus axis data and put it in a pandas column\n",
    "def indexit(row):\n",
    "    joy_mat = [row.JOY_ax0, row.JOY_ax1, row.JOY_ax2]\n",
    "    return joy_mat[int(row.ax)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acd97366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels_and_initial_feature(df):\n",
    "    # IC = 1\n",
    "    # EC = 2, 4, 5\n",
    "    # NC = 3, 6, 7\n",
    "    # NR = 9\n",
    "    # (IC) - sham (do not use) = 8\n",
    "    # (NC) - sham (do not use) = 10\n",
    "\n",
    "    # Just to confirm, what are the unique values of res_type\n",
    "    df.res_type.value_counts(ascending=True)\n",
    "\n",
    "    # Construction of SD_label : How do we define disorientation?\n",
    "\n",
    "    # Way 0 : lenient\n",
    "    # 0 = If participates got the result CORRECT for the trial, they were NOT disoriented. (IC, EC)\n",
    "    # 1 = If participates got the result WRONG or did not respond, they were disoriented. (NC, NR)\n",
    "\n",
    "    idx_NDS = df.index[(df.res_type == 1) | (df.res_type == 2) | (df.res_type == 4) | (df.res_type == 5)].to_list()\n",
    "    idx_DS = df.index[(df.res_type == 3) | (df.res_type == 6) | (df.res_type == 7) | (df.res_type == 9)].to_list()\n",
    "    df.lenient = ''  # define a new column , rows 8 and 10 will be NaN, need to do dropna for rows\n",
    "    df.loc[idx_NDS, 'lenient'] = 0\n",
    "    df.loc[idx_DS, 'lenient'] = 1\n",
    "\n",
    "    # -------------------------------------\n",
    "\n",
    "    # Way 1 : strict simple\n",
    "    # 0 = If participants got the result initially CORRECT, they were NOT disoriented. (IC)\n",
    "    # 1 = If participants were WRONG at any point, they were disoriented. (EC, NC, NR)\n",
    "\n",
    "    idx_NDS = df.index[(df.res_type == 1)].to_list()\n",
    "    idx_DS = df.index[(df.res_type == 2) | (df.res_type == 4) | (df.res_type == 5) | (df.res_type == 3) | (df.res_type == 6) | (df.res_type == 7) | (df.res_type == 9)].to_list()\n",
    "    df.strict = ''  # define a new column\n",
    "    df.loc[idx_NDS, 'strict'] = 0\n",
    "    df.loc[idx_DS, 'strict'] = 1\n",
    "\n",
    "    # Way 2 : st_complex\n",
    "    # 0 = If participants got the result initially CORRECT, they were NOT disoriented. (IC)\n",
    "    # 1 = If participants got the result eventually CORRECT, they were MILDLY disoriented. (EC)\n",
    "    # 2 = If participants were WRONG for the trial, they were disoriented. (NC, NR)\n",
    "\n",
    "    idx_NDS = df.index[(df.res_type == 1)].to_list()\n",
    "    idx_MDS = df.index[(df.res_type == 2) | (df.res_type == 4) | (df.res_type == 5)].to_list()\n",
    "    idx_DS = df.index[(df.res_type == 3) | (df.res_type == 6) | (df.res_type == 7) | (df.res_type == 9)].to_list()\n",
    "    df.st_complex = ''  # define a new column\n",
    "    df.loc[idx_NDS, 'st_complex'] = 0\n",
    "    df.loc[idx_MDS, 'st_complex'] = 1\n",
    "    df.loc[idx_DS, 'st_complex'] = 2\n",
    "\n",
    "    # -------------------------------------\n",
    "\n",
    "    # Create features :  (1) position\n",
    "    df['joy_stim'] = df.apply(indexit, axis='columns')  # fill in joy_stim\n",
    "    \n",
    "    # -------------------------------------\n",
    "\n",
    "    # Make DataFrame for trial start-stop index\n",
    "    # Cut the data up per trial across subjects\n",
    "    tr_vec = df.tr.to_numpy()\n",
    "\n",
    "    st = [0]\n",
    "    ender = []\n",
    "    for i in range(len(tr_vec)-1):\n",
    "        if tr_vec[i] != tr_vec[i+1]:\n",
    "            st = st + [i+1]\n",
    "            ender = ender + [i]\n",
    "    ender = ender + [len(tr_vec)-1]\n",
    "\n",
    "    # See start-stop index clearly\n",
    "    e0 = np.reshape(st, (len(st),1))\n",
    "    e1 = np.reshape(ender, (len(st),1))\n",
    "    data = np.ravel(e0), np.ravel(e1)\n",
    "    data = np.transpose(data)\n",
    "    columns = ['stind', 'endind']\n",
    "    temp = pd.DataFrame(data=data, columns=columns)\n",
    "\n",
    "    # -------------------------------------\n",
    "\n",
    "    # Find the longest trial signal in df_rot['joy_stim']\n",
    "    temp['diff'] = temp.endind - temp.stind\n",
    "    temp['timediff'] = [df.time.iloc[temp.endind[i]] - df.time.iloc[temp.stind[i]] for i in range(len(temp.endind))]\n",
    "    outmin = temp['diff'].min()\n",
    "    outmax = temp['diff'].max()\n",
    "\n",
    "    tomin = temp['timediff'][(temp['diff'] == outmin)]\n",
    "    tomax = temp['timediff'][(temp['diff'] == outmax)]\n",
    "    # print('outmin : ', outmin, 't :', tomin)\n",
    "    # print('outmax : ', outmax, 't :', tomax)\n",
    "\n",
    "    # -------------------------------------\n",
    "\n",
    "    # Interpolate : make each trial the same number of data points\n",
    "    from scipy.interpolate import interp1d\n",
    "    feat0 = []\n",
    "    t_feat0 = []\n",
    "    y1_feat0 = []\n",
    "    y2_feat0 = []\n",
    "    y3_feat0 = []\n",
    "    for i in range(len(temp.stind)):\n",
    "        \n",
    "        # X\n",
    "        sSIG = df['joy_stim'][temp.stind.iloc[i]:temp.endind.iloc[i]].to_numpy()\n",
    "        t_sSIG = df['time'][temp.stind.iloc[i]:temp.endind.iloc[i]].to_numpy()\n",
    "\n",
    "        # labels\n",
    "        y1 = df['lenient'][temp.stind.iloc[i]:temp.endind.iloc[i]].to_numpy()\n",
    "        y2 = df['strict'][temp.stind.iloc[i]:temp.endind.iloc[i]].to_numpy()\n",
    "        y3 = df['st_complex'][temp.stind.iloc[i]:temp.endind.iloc[i]].to_numpy()\n",
    "        \n",
    "        # Check if trial data is less than the maximum length\n",
    "        if len(df['joy_stim'][temp.stind.iloc[i]:temp.endind.iloc[i]]) < outmax:\n",
    "            \n",
    "            # The trial length is different so interpolate the time-series to make them the same length signal \n",
    "            x = np.linspace(sSIG[0], len(sSIG), num=len(sSIG), endpoint=True)\n",
    "            xnew = np.linspace(sSIG[0], len(sSIG), num=outmax, endpoint=True)\n",
    "\n",
    "            # joystick on stim\n",
    "            f = interp1d(x, sSIG)\n",
    "            sSIGl = f(xnew)\n",
    "\n",
    "            # time\n",
    "            f = interp1d(x, t_sSIG)\n",
    "            t_sSIGl = f(xnew)\n",
    "\n",
    "            # y1\n",
    "            f = interp1d(x, y1)\n",
    "            y1_sSIGl = f(xnew)\n",
    "\n",
    "            # y2\n",
    "            f = interp1d(x, y2)\n",
    "            y2_sSIGl = f(xnew)\n",
    "\n",
    "            # y3\n",
    "            f = interp1d(x, y3)\n",
    "            y3_sSIGl = f(xnew)\n",
    "\n",
    "            # python : you can not create a matrix in real-time in pandas\n",
    "            # you only assign the full matrix at the end\n",
    "            # (0) position\n",
    "            feat0 = feat0 + [sSIGl]\n",
    "            t_feat0 = t_feat0 + [t_sSIGl]\n",
    "            y1_feat0 = y1_feat0 + [np.ravel(y1_sSIGl)]\n",
    "            y2_feat0 = y2_feat0 + [np.ravel(y2_sSIGl)]\n",
    "            y3_feat0 = y3_feat0 + [np.ravel(y3_sSIGl)]\n",
    "            \n",
    "            del x, f, sSIGl, t_sSIGl, y1_sSIGl, y2_sSIGl, y3_sSIGl\n",
    "        else:\n",
    "            feat0 = feat0 + [sSIG]\n",
    "            t_feat0 = t_feat0 + [t_sSIG]\n",
    "            y1_feat0 = y1_feat0 + [np.ravel(y1)]\n",
    "            y2_feat0 = y2_feat0 + [np.ravel(y2)]\n",
    "            y3_feat0 = y3_feat0 + [np.ravel(y3)]\n",
    "\n",
    "    # Clean up\n",
    "    del df\n",
    "    # -------------------------------------\n",
    "    \n",
    "    return feat0, t_feat0, y1_feat0, y2_feat0, y3_feat0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "989e826c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def feature_creation_preprocessing(feat0, t_feat0):\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    # ----------------\n",
    "    # Make your features\n",
    "    # ----------------\n",
    "    norm_feat0 = []\n",
    "    norm_feat1 = []\n",
    "    norm_feat2 = []\n",
    "    norm_feat3 = []\n",
    "    feat4 = []\n",
    "    feat4_1 = []\n",
    "    \n",
    "    n = 4   # filter order\n",
    "    fs = 250 # data sampling frequency (Hz)\n",
    "    fcc = 10  # Cut-off frequency of the filter\n",
    "    w = fcc / (fs / 2) # Normalize the frequency\n",
    "    b, a = signal.butter(n, w, 'low')  # 3rd order\n",
    "\n",
    "    # Need to find when one trial starts and end - take derivative from start-stop periods\n",
    "    for i in range(len(feat0)):\n",
    "\n",
    "        if i == 0:\n",
    "            plotORnot = 0 # 1\n",
    "        else:\n",
    "            plotORnot = 0\n",
    "\n",
    "        # (0) position\n",
    "        scaled_data0 = scale_feature_data(feat0[i], plotORnot)\n",
    "        # print('shape of scaled_data0 : ', np.array(scaled_data0).shape)\n",
    "        temp0 = normal_distribution_feature_data(scaled_data0, plotORnot)\n",
    "        norm_feat0 = norm_feat0 + [temp0]\n",
    "\n",
    "        # (1) velocity\n",
    "        vel = numderiv(feat0[i], t_feat0[i])\n",
    "        scaled_data1 = scale_feature_data(vel, plotORnot)\n",
    "        temp1 = normal_distribution_feature_data(scaled_data1, plotORnot)\n",
    "        norm_feat1 = norm_feat1 + [temp1]\n",
    "\n",
    "        # (2) acceleration\n",
    "        # the signal is noisy, there is a numerical defect : 1) tried changing the time to see if it would smooth out (does not work), 2) filter\n",
    "\n",
    "        # shift time to the right by one\n",
    "        # t_shift = []\n",
    "        # for b in range(len(t_feat0[i])-1):\n",
    "            # t_shift.append(t_feat0[i][b+1])\n",
    "        # t_shift.append(t_feat0[i][-1])\n",
    "        # acc = numderiv(vel, t_shift)\n",
    "        acc = numderiv(vel, t_feat0[i])\n",
    "        filtacc = signal.filtfilt(b, a, acc)\n",
    "\n",
    "        scaled_data2 = scale_feature_data(filtacc, plotORnot)\n",
    "        temp2 = normal_distribution_feature_data(scaled_data2, plotORnot)\n",
    "        norm_feat2 = norm_feat2 + [temp2]\n",
    "\n",
    "        # (3) frequency response of joystick via FFT\n",
    "        Xn_mag_in, Xn_phase_in, omeg_in, Xn_mag_half_db_in, Xn_phase_half_in, omg_half_in = get_freqresp_mag_phase(feat0[i], t_feat0[i], 0.1)\n",
    "        scaled_data3 = scale_feature_data(Xn_mag_in, plotORnot)\n",
    "        temp3 = normal_distribution_feature_data(scaled_data3, plotORnot)\n",
    "        norm_feat3 = norm_feat3 + [temp3]\n",
    "\n",
    "        # (4) frequency of joystick : frequency response db cutoff\n",
    "        dp_jump = 0.5  # this is half the height of the scaled signal scaled_data0\n",
    "        fc = freq_from_sig_timecounting(scaled_data0, t_feat0[i], 0.1, dp_jump, plotORnot=0)\n",
    "        fc1 = freq_from_sig_freqresp(feat0[i], t_feat0[i], ts=0.1, plotORnot=0)\n",
    "        temp4 = fc*np.ones((len(feat0[i])))\n",
    "        feat4 = feat4 + [temp4]\n",
    "\n",
    "        temp4_1 = fc1*np.ones((len(feat0[i])))\n",
    "        feat4_1 = feat4_1 + [temp4_1]\n",
    "\n",
    "        # print('fc = ', fc, ', fc1 = ', fc1)\n",
    "\n",
    "        # (5) integral error with respect to theoretical stimuli (this is really part of the control response)\n",
    "        # diff = feat0[i] - theoretical_stim[i]\n",
    "        # feat5 = feat5 + [diff]\n",
    "\n",
    "    # Clean up\n",
    "    del n, fs, fcc, w, b, a, plotORnot\n",
    "    del feat0, t_feat0, scaled_data0, vel, scaled_data1, scaled_data2, filtacc\n",
    "    del Xn_mag_in, Xn_phase_in, omeg_in, Xn_mag_half_db_in, Xn_phase_half_in, omg_half_in, scaled_data3\n",
    "    del fc, fc1\n",
    "    # ----------------\n",
    "\n",
    "    end = time.time()\n",
    "    print('Elasped time for feature processing : ', end - start)\n",
    "\n",
    "    return norm_feat0, norm_feat1, norm_feat2, norm_feat3, feat4, feat4_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8f2dee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_data_2makeclasses_equivalent(df_test):\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Remove nan value per row\n",
    "    # df_test.dropna(axis=0)\n",
    "    # OR\n",
    "    df_test_noNan = my_dropna_python(df_test)\n",
    "\n",
    "    #del df_test\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # DataFrame is organized : [X, y]  where each of X columns are nested arrays\n",
    "\n",
    "    num_of_feat = df_test_noNan.shape[1] - 1\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Add on a scalar column of the y label, for searching\n",
    "    val = []\n",
    "    for i in range(len(df_test_noNan.iloc[:,-1])):\n",
    "        val.append(df_test_noNan.iloc[i,-1][0])\n",
    "    # print('length of val : ', len(val))\n",
    "    valS = pd.Series(val)\n",
    "\n",
    "    df_test_noNan = pd.concat([df_test_noNan, valS], axis=1)\n",
    "    df_test_noNan = df_test_noNan.rename({0: 'y_scalar'}, axis=1)\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Count the unique values in the label\n",
    "    df_vc = pd.DataFrame(df_test_noNan.y_scalar.value_counts())\n",
    "    df_vc.head()\n",
    "    counted_value = df_vc.iloc[:,0].to_numpy()\n",
    "    count_index = df_vc.iloc[:,0].index.to_numpy()\n",
    "    # print('Before sorting counted_value : ', counted_value)\n",
    "    # print('Before sorting count_index : ', count_index)\n",
    "    \n",
    "    # Sort counted_value by count_index; in ascending order\n",
    "    sind = np.argsort(count_index)\n",
    "    count_index = count_index[sind]\n",
    "    counted_value = counted_value[sind]\n",
    "    print('After sorting counted_value : ', counted_value)\n",
    "    print('After sorting count_index : ', count_index)\n",
    "    \n",
    "    out = {}\n",
    "    for i, val in enumerate(counted_value):\n",
    "        out[count_index[i]] = val\n",
    "    print('Original class count : ', out)\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Determine how much to pad each class label\n",
    "    needed_samps_class = np.max(counted_value) - counted_value\n",
    "    print('needed_samps_class : ', needed_samps_class)\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Confirm that there are no nan values\n",
    "    out = df_test_noNan.isnull().values.any()\n",
    "    print('Are there nan valeus in the data : ', out)\n",
    "\n",
    "    # ----------------\n",
    "    \n",
    "    print('shape of dataframe before padding : ', df_test_noNan.shape)\n",
    "\n",
    "    # ----------------\n",
    "    # Pad the DataFrame\n",
    "    n_classes = len(count_index)\n",
    "    n_samples = len(df_test_noNan)\n",
    "\n",
    "    df_2add_on = pd.DataFrame()\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        #print('i : ', i)\n",
    "        # Pad short length classes\n",
    "        for j in range(needed_samps_class[i]):\n",
    "            #print('j : ', j) \n",
    "            flag = 0\n",
    "            while flag == 0:\n",
    "                permvec = np.random.permutation(n_samples)\n",
    "                index = permvec[0]  #random choosen index\n",
    "\n",
    "                # look for each class\n",
    "                if i == int(df_test_noNan.y_scalar.iloc[index]):\n",
    "                    #print('Class match was found : i = ', i, ', data index = ', int(df_test_noNan.y_scalar.iloc[index]), ', index = ', index)\n",
    "                    \n",
    "                    # Append the data with padded data entry\n",
    "                    data = []\n",
    "                    for nf in range(num_of_feat+2):\n",
    "                        if nf == num_of_feat+1:\n",
    "                            # The last column is a scalar - can not use list\n",
    "                            data.append([df_test_noNan.iloc[index, nf]])\n",
    "                        else:\n",
    "                            data.append([list(df_test_noNan.iloc[index, nf])])\n",
    "\n",
    "                    df_row = pd.DataFrame(data=data)\n",
    "                    df_row = df_row.T # or df1.transpose()\n",
    "                    df_2add_on = pd.concat([df_2add_on, df_row], axis=0)\n",
    "\n",
    "                    flag = 1 # to brake while\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    df_2add_on = df_2add_on.reset_index(drop=True)  # reset index : delete the old index column\n",
    "\n",
    "    dfl = {}\n",
    "    for nf in range(num_of_feat+2):\n",
    "        if nf == num_of_feat+1:\n",
    "            dfl[nf] = 'y_scalar'\n",
    "        elif nf == num_of_feat:\n",
    "            dfl[nf] = 'y'\n",
    "        else:\n",
    "            dfl[nf] = '%d' % (nf)\n",
    "\n",
    "    df_2add_on = df_2add_on.rename(dfl, axis=1)\n",
    "    \n",
    "    print('shape of dataframe to add to original dataframe: ', df_2add_on.shape)\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # want to arrange the dataframe with respect to rows (stack on top of the other): so axis=0 \n",
    "    # OR think of it as the rows of the df change so you put axis=0 for rows\n",
    "    df_test2 = pd.concat([df_test_noNan, df_2add_on], axis=0)\n",
    "    df_test2 = df_test2.reset_index(drop=True)  # reset index : delete the old index column\n",
    "\n",
    "    print('shape of padded dataframe (original + toadd) : ', df_test2.shape)\n",
    "    \n",
    "    del df_test_noNan, df_2add_on\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Final check of class balance\n",
    "    df_vc = pd.DataFrame(df_test2.y_scalar.value_counts())\n",
    "    df_vc.head()\n",
    "    counted_value = df_vc.iloc[:,0].to_numpy()\n",
    "    count_index = df_vc.iloc[:,0].index.to_numpy()\n",
    "    # print('Before sorting counted_value : ', counted_value)\n",
    "    # print('Before sorting count_index : ', count_index)\n",
    "    \n",
    "    # Sort counted_value by count_index; in ascending order\n",
    "    sind = np.argsort(count_index)\n",
    "    count_index = count_index[sind]\n",
    "    counted_value = counted_value[sind]\n",
    "    print('After sorting counted_value : ', counted_value)\n",
    "    print('After sorting count_index : ', count_index)\n",
    "    \n",
    "    out = {}\n",
    "    for i, val in enumerate(counted_value):\n",
    "        out[count_index[i]] = val\n",
    "    print('New class count : ', out)\n",
    "\n",
    "    # Determine how much to pad each class label\n",
    "    needed_samps_class = np.max(counted_value) - counted_value\n",
    "    print('needed_samps_class : ', needed_samps_class)\n",
    "\n",
    "    # ----------------\n",
    "    \n",
    "    # After the final check, drop the y_scalar column\n",
    "    df_test2 = df_test2.drop('y_scalar', 1)  # 1 is the axis number (0 for rows and 1 for columns)\n",
    "    \n",
    "    # ----------------\n",
    "\n",
    "    return df_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddd1f883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting columns from featlab: pandas\n",
    "plotORnot = 0\n",
    "if plotORnot == 1:\n",
    "    # Plot all the data (data is too large : can not do it)\n",
    "    pd.options.plotting.backend = \"plotly\"\n",
    "    fig = featlab.loc[:,8].plot()\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2278094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df_2_XYtraintest(featlab_new, fea, ynum, ovtot):\n",
    "    # ----------------\n",
    "    # Check to see if each y label has a balanced number of class samples\n",
    "    # ----------------\n",
    "    num_of_feat = featlab_new.shape[1] - 1 #6\n",
    "    test_size = 0.25 # default\n",
    "\n",
    "    # ----------------\n",
    "    # Order of which features to use in a model : 1) All features, \n",
    "    # 2) first 3 from permutation_importance, \n",
    "    # 3) first 2 from permutation_importance, \n",
    "    # 4) first feature from permutation_importance\n",
    "    if fea == 0:\n",
    "        # 1) All features\n",
    "        X_part = list(range(num_of_feat))\n",
    "        y_part = num_of_feat   # Add label y\n",
    "        desired_col = make_a_properlist([X_part, y_part])\n",
    "    elif fea == 1:\n",
    "        # 2) first 3 from permutation_importance\n",
    "        desired_col = []\n",
    "        # Add features for X\n",
    "        for oo in range(3): \n",
    "            if oo < len(ovtot):\n",
    "                if ovtot[oo][0] == 'joy':\n",
    "                    desired_col.append(0)\n",
    "                elif ovtot[oo][0] == 'joy1derv':\n",
    "                    desired_col.append(1)\n",
    "                elif ovtot[oo][0] == 'joy2derv':\n",
    "                    desired_col.append(2)\n",
    "                elif ovtot[oo][0] == 'fres':\n",
    "                    desired_col.append(3)\n",
    "                elif ovtot[oo][0] == 'freq_t':\n",
    "                    desired_col.append(4)\n",
    "                elif ovtot[oo][0] == 'freq_fres':\n",
    "                    desired_col.append(5)\n",
    "            else:\n",
    "                z = desired_col[-1]\n",
    "                desired_col.append(z)\n",
    "        \n",
    "        desired_col.append(num_of_feat)  # Add label y\n",
    "        desired_col = np.ravel(desired_col)\n",
    "    elif fea == 2:\n",
    "        # 3) first 2 from permutation_importance\n",
    "        desired_col = []\n",
    "        for oo in range(2):\n",
    "            if oo < len(ovtot):\n",
    "                if ovtot[oo][0] == 'joy':\n",
    "                    desired_col.append(0)\n",
    "                elif ovtot[oo][0] == 'joy1derv':\n",
    "                    desired_col.append(1)\n",
    "                elif ovtot[oo][0] == 'joy2derv':\n",
    "                    desired_col.append(2)\n",
    "                elif ovtot[oo][0] == 'fres':\n",
    "                    desired_col.append(3)\n",
    "                elif ovtot[oo][0] == 'freq_t':\n",
    "                    desired_col.append(4)\n",
    "                elif ovtot[oo][0] == 'freq_fres':\n",
    "                    desired_col.append(5)\n",
    "            else:\n",
    "                z = desired_col[-1]\n",
    "                desired_col.append(z)\n",
    "        \n",
    "        desired_col.append(num_of_feat)  # Add label y\n",
    "        desired_col = np.ravel(desired_col)\n",
    "    elif fea == 3:\n",
    "        # 4) first feature from permutation_importance\n",
    "        if ovtot[0][0] == 'joy':\n",
    "            vvv = 0\n",
    "        elif ovtot[0][0] == 'joy1derv':\n",
    "            vvv = 1\n",
    "        elif ovtot[0][0] == 'joy2derv':\n",
    "            vvv = 2\n",
    "        elif ovtot[0][0] == 'fres':\n",
    "            vvv = 3\n",
    "        elif ovtot[0][0] == 'freq_t':\n",
    "            vvv = 4\n",
    "        elif ovtot[0][0] == 'freq_fres':\n",
    "            vvv = 5\n",
    "        y_part = num_of_feat    # Add label y\n",
    "        desired_col = [vvv, y_part]\n",
    "\n",
    "    print('desired_col : ', desired_col)  # where first columns are X, and last column is y\n",
    "    df_y = featlab_new.iloc[:, desired_col]\n",
    "\n",
    "    # ----------------\n",
    "    \n",
    "    print('shape of df_y : ', df_y.shape)\n",
    "    df_y.head()\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Divide DataFrame into X and y\n",
    "    a = df_y.shape[1]\n",
    "    print('a : ', a)\n",
    "\n",
    "    X = df_y.iloc[:,0:a-1]\n",
    "    X = X.to_numpy()\n",
    "    print('shape of X : ', X.shape)\n",
    "\n",
    "    y = df_y.iloc[:,a-1:a].to_numpy()\n",
    "    y = np.reshape(y, (len(y), 1))\n",
    "    print('length of y : ', len(y))\n",
    "\n",
    "    # Check if the y label is correct (only 0's and 1's)\n",
    "    print('y :', y)\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Split the X an y data into test and train\n",
    "    seed = 0\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, y, random_state = seed, test_size = test_size)\n",
    "    print('X_train : ', len(X_train))\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    print('shape of X_train : ', X_train.shape)\n",
    "    Y_train = np.array(Y_train)\n",
    "    print('shape of Y_train : ', Y_train.shape)\n",
    "    X_test = np.array(X_test)\n",
    "    print('shape of X_test : ', X_test.shape)\n",
    "    Y_test = np.array(Y_test)\n",
    "    print('shape of Y_test : ', Y_test.shape)\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # Ensure that Y_train and Y_test are integers\n",
    "    Y_test = Y_test.astype(int)\n",
    "    Y_train = Y_train.astype(int)\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test, desired_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5e031cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def classify(featlab_new, ynum):\n",
    "    \n",
    "    # Loop over each model to test\n",
    "    # 0,1,2,3,4,5,6,\n",
    "    # 7 SVM can not converge\n",
    "    for wm in [2, 3, 4]:\n",
    "        res_permod = []\n",
    "        \n",
    "        ovtot = []\n",
    "        \n",
    "        for fea in range(4):\n",
    "            print('fea : ', fea)\n",
    "\n",
    "            # Selects the feature columns from the Dataframe per fea \n",
    "            X_train, X_test, Y_train_1D, Y_test_1D, desired_col = preprocess_df_2_XYtraintest(featlab_new, fea, ynum, ovtot)\n",
    "            print('preprocess_df_2_XYtraintest finished ')\n",
    "            \n",
    "            # Determine if classes are binary or multiclass:\n",
    "            class_len = len(np.unique(Y_train_1D))\n",
    "            if class_len <= 2:\n",
    "                cltype = 'binary'\n",
    "            elif class_len > 2:\n",
    "                cltype = 'multi'\n",
    "            \n",
    "            print('class_len : ', class_len)\n",
    "            print('cltype : ', cltype)\n",
    "            \n",
    "            if wm == 0:\n",
    "                # Stochastic Gradient Descent\n",
    "                m_name = 'SGD'\n",
    "                if cltype == 'binary':\n",
    "                    model, Y_train_1D_predict, Y_test_1D_predict, Y_train_bin_pp, Y_test_bin_pp, Y_train_1D_score, Y_test_1D_score = binary_stochastic_gradient_descent(X_train, X_test, Y_train_1D, Y_test_1D)\n",
    "                    value_pack_train = evaluation_methods_binary_class(model, X_train, Y_train_1D, Y_train_1D_predict, Y_train_bin_pp, Y_train_1D_score)\n",
    "                    value_pack_test = evaluation_methods_binary_class(model, X_test, Y_test_1D, Y_test_1D_predict, Y_test_bin_pp, Y_test_1D_score)\n",
    "                elif cltype == 'multi':\n",
    "                    model, Y_train_1D_predict, Y_test_1D_predict, Y_train_bin_pp, Y_test_bin_pp, Y_train_bin_score, Y_test_bin_score = multiclass_stochastic_gradient_descent(X_train, X_test, Y_train_1D, Y_test_1D)\n",
    "                    value_pack_train = evaluation_methods_multi_class_1D(model, X_train, Y_train_1D, Y_train_1D_predict, Y_train_bin_pp, Y_train_bin_score)\n",
    "                    value_pack_test = evaluation_methods_multi_class_1D(model, X_test, Y_test_1D, Y_test_1D_predict, Y_test_bin_pp, Y_test_bin_score)\n",
    "            elif wm == 1:\n",
    "                # Linear Discriminant Analysis (LDA)\n",
    "                m_name = 'LDA'\n",
    "                if cltype == 'binary':\n",
    "                    model, Y_train_1D_predict, Y_test_1D_predict, Y_train_bin_pp, Y_test_bin_pp, Y_train_1D_score, Y_test_1D_score = binary_LDA_classifier(X_train, X_test, Y_train_1D, Y_test_1D)\n",
    "                    value_pack_train = evaluation_methods_binary_class(model, X_train, Y_train_1D, Y_train_1D_predict, Y_train_bin_pp, Y_train_1D_score)\n",
    "                    value_pack_test = evaluation_methods_binary_class(model, X_test, Y_test_1D, Y_test_1D_predict, Y_test_bin_pp, Y_test_1D_score)\n",
    "                elif cltype == 'multi':\n",
    "                    model, Y_train_1D_predict, Y_test_1D_predict, Y_train_bin_pp, Y_test_bin_pp, Y_train_bin_score, Y_test_bin_score = multiclass_LDA_classifier(X_train, X_test, Y_train_1D, Y_test_1D)\n",
    "                    value_pack_train = evaluation_methods_multi_class_1D(model, X_train, Y_train_1D, Y_train_1D_predict, Y_train_bin_pp, Y_train_bin_score)\n",
    "                    value_pack_test = evaluation_methods_multi_class_1D(model, X_test, Y_test_1D, Y_test_1D_predict, Y_test_bin_pp, Y_test_bin_score)\n",
    "            elif wm == 2:\n",
    "                # RandomForest\n",
    "                m_name = 'RF'\n",
    "                if cltype == 'binary':\n",
    "                    model, Y_train_1D_predict, Y_test_1D_predict, Y_train_bin_pp, Y_test_bin_pp, Y_train_1D_score, Y_test_1D_score = binary_RandomForest(X_train, X_test, Y_train_1D, Y_test_1D)\n",
    "                    value_pack_train = evaluation_methods_binary_class(model, X_train, Y_train_1D, Y_train_1D_predict, Y_train_bin_pp, Y_train_1D_score)\n",
    "                    value_pack_test = evaluation_methods_binary_class(model, X_test, Y_test_1D, Y_test_1D_predict, Y_test_bin_pp, Y_test_1D_score)\n",
    "                elif cltype == 'multi':\n",
    "                    model, Y_train_1D_predict, Y_test_1D_predict, Y_train_bin_pp, Y_test_bin_pp, Y_train_bin_score, Y_test_bin_score = multiclass_RandomForest_1Dinput(X_train, X_test, Y_train_1D, Y_test_1D)\n",
    "                    value_pack_train = evaluation_methods_multi_class_1D(model, X_train, Y_train_1D, Y_train_1D_predict, Y_train_bin_pp, Y_train_bin_score)\n",
    "                    value_pack_test = evaluation_methods_multi_class_1D(model, X_test, Y_test_1D, Y_test_1D_predict, Y_test_bin_pp, Y_test_bin_score)\n",
    "            elif wm == 3:\n",
    "                # Gradient Boosting Classifier (gradient descent w/ logistic regression cost function)\n",
    "                m_name = 'GBC'\n",
    "                if cltype == 'binary':\n",
    "                    model, Y_train_1D_predict, Y_test_1D_predict, Y_train_bin_pp, Y_test_bin_pp, Y_train_1D_score, Y_test_1D_score = binary_GradientBoostingClassifier(X_train, X_test, Y_train_1D, Y_test_1D)\n",
    "                    value_pack_train = evaluation_methods_binary_class(model, X_train, Y_train_1D, Y_train_1D_predict, Y_train_bin_pp, Y_train_1D_score)\n",
    "                    value_pack_test = evaluation_methods_binary_class(model, X_test, Y_test_1D, Y_test_1D_predict, Y_test_bin_pp, Y_test_1D_score)\n",
    "                elif cltype == 'multi':\n",
    "                    model, Y_train_1D_predict, Y_test_1D_predict, Y_train_bin_pp, Y_test_bin_pp, Y_train_bin_score, Y_test_bin_score = multiclass_GradientBoostingClassifier(X_train, X_test, Y_train_1D, Y_test_1D)\n",
    "                    value_pack_train = evaluation_methods_multi_class_1D(model, X_train, Y_train_1D, Y_train_1D_predict, Y_train_bin_pp, Y_train_bin_score)\n",
    "                    value_pack_test = evaluation_methods_multi_class_1D(model, X_test, Y_test_1D, Y_test_1D_predict, Y_test_bin_pp, Y_test_bin_score)\n",
    "            elif wm == 4:\n",
    "                # Decision Tree Classifier\n",
    "                m_name = 'DT'\n",
    "                if cltype == 'binary':\n",
    "                    model, Y_train_1D_predict, Y_test_1D_predict, Y_train_bin_pp, Y_test_bin_pp, Y_train_1D_score, Y_test_1D_score = binary_Decision_Tree_Classifier(X_train, X_test, Y_train_1D, Y_test_1D)\n",
    "                    value_pack_train = evaluation_methods_binary_class(model, X_train, Y_train_1D, Y_train_1D_predict, Y_train_bin_pp, Y_train_1D_score)\n",
    "                    value_pack_test = evaluation_methods_binary_class(model, X_test, Y_test_1D, Y_test_1D_predict, Y_test_bin_pp, Y_test_1D_score)\n",
    "                elif cltype == 'multi':\n",
    "                    model, Y_train_1D_predict, Y_test_1D_predict, Y_train_bin_pp, Y_test_bin_pp, Y_train_bin_score, Y_test_bin_score = multiclass_Decision_Tree_Classifier(X_train, X_test, Y_train_1D, Y_test_1D)\n",
    "                    value_pack_train = evaluation_methods_multi_class_1D(model, X_train, Y_train_1D, Y_train_1D_predict, Y_train_bin_pp, Y_train_bin_score)\n",
    "                    value_pack_test = evaluation_methods_multi_class_1D(model, X_test, Y_test_1D, Y_test_1D_predict, Y_test_bin_pp, Y_test_bin_score)\n",
    "            elif wm == 5:\n",
    "                # Multilayer perceptron (MLP)/neural network (Deep Learning) : logistic regression NN\n",
    "                m_name = 'MLP'\n",
    "                if cltype == 'binary':\n",
    "                    model, Y_train_1D_predict, Y_test_1D_predict, Y_train_bin_pp, Y_test_bin_pp, Y_train_1D_score, Y_test_1D_score = binary_multilayer_perceptron(X_train, X_test, Y_train_1D, Y_test_1D)\n",
    "                    value_pack_train = evaluation_methods_binary_class(model, X_train, Y_train_1D, Y_train_1D_predict, Y_train_bin_pp, Y_train_1D_score)\n",
    "                    value_pack_test = evaluation_methods_binary_class(model, X_test, Y_test_1D, Y_test_1D_predict, Y_test_bin_pp, Y_test_1D_score)\n",
    "                elif cltype == 'multi':\n",
    "                    model, Y_train_1D_predict, Y_test_1D_predict, Y_train_bin_pp, Y_test_bin_pp, Y_train_bin_score, Y_test_bin_score = multiclass_multilayer_perceptron_1Dinput(X_train, X_test, Y_train_1D, Y_test_1D)\n",
    "                    value_pack_train = evaluation_methods_multi_class_1D(model, X_train, Y_train_1D, Y_train_1D_predict, Y_train_bin_pp, Y_train_bin_score)\n",
    "                    value_pack_test = evaluation_methods_multi_class_1D(model, X_test, Y_test_1D, Y_test_1D_predict, Y_test_bin_pp, Y_test_bin_score)\n",
    "            elif wm == 6:\n",
    "                # Gaussian Naive Bayes\n",
    "                m_name = 'GNB'\n",
    "                if cltype == 'binary':\n",
    "                    model, Y_train_1D_predict, Y_test_1D_predict, Y_train_bin_pp, Y_test_bin_pp, Y_train_1D_score, Y_test_1D_score = binary_gaussian_naive_bayes(X_train, X_test, Y_train_1D, Y_test_1D)\n",
    "                    value_pack_train = evaluation_methods_binary_class(model, X_train, Y_train_1D, Y_train_1D_predict, Y_train_bin_pp, Y_train_1D_score)\n",
    "                    value_pack_test = evaluation_methods_binary_class(model, X_test, Y_test_1D, Y_test_1D_predict, Y_test_bin_pp, Y_test_1D_score)\n",
    "                elif cltype == 'multi':\n",
    "                    model, Y_train_1D_predict, Y_test_1D_predict, Y_train_bin_pp, Y_test_bin_pp, Y_train_bin_score, Y_test_bin_score = multiclass_gaussian_naive_bayes(X_train, X_test, Y_train_1D, Y_test_1D)\n",
    "                    value_pack_train = evaluation_methods_multi_class_1D(model, X_train, Y_train_1D, Y_train_1D_predict, Y_train_bin_pp, Y_train_bin_score)\n",
    "                    value_pack_test = evaluation_methods_multi_class_1D(model, X_test, Y_test_1D, Y_test_1D_predict, Y_test_bin_pp, Y_test_bin_score)\n",
    "            elif wm == 7:\n",
    "                # Support Vector Machine (NuSVC)\n",
    "                m_name = 'NuSVC'\n",
    "                if cltype == 'binary':\n",
    "                    model, Y_train_1D_predict, Y_test_1D_predict, Y_train_bin_pp, Y_test_bin_pp, Y_train_1D_score, Y_test_1D_score = binary_svm_NuSVC(X_train, X_test, Y_train_1D, Y_test_1D)\n",
    "                    value_pack_train = evaluation_methods_binary_class(model, X_train, Y_train_1D, Y_train_1D_predict, Y_train_bin_pp, Y_train_1D_score)\n",
    "                    value_pack_test = evaluation_methods_binary_class(model, X_test, Y_test_1D, Y_test_1D_predict, Y_test_bin_pp, Y_test_1D_score)\n",
    "                elif cltype == 'multi':\n",
    "                    model, Y_train_1D_predict, Y_test_1D_predict, Y_train_bin_pp, Y_test_bin_pp, Y_train_bin_score, Y_test_bin_score = multiclass_svm_NuSVC(X_train, X_test, Y_train_1D, Y_test_1D)\n",
    "                    value_pack_train = evaluation_methods_multi_class_1D(model, X_train, Y_train_1D, Y_train_1D_predict, Y_train_bin_pp, Y_train_bin_score)\n",
    "                    value_pack_test = evaluation_methods_multi_class_1D(model, X_test, Y_test_1D, Y_test_1D_predict, Y_test_bin_pp, Y_test_bin_score)\n",
    "                    \n",
    "            print('m_name : ', m_name)\n",
    "            \n",
    "            # print('value_pack_train : ', value_pack_train)\n",
    "            print('value_pack_test : ', value_pack_test)\n",
    "            \n",
    "            # The cross validation sometimes returned nan, whereas the direct calculation returned a resonable value\n",
    "            # Thus, taking direct calculation. \n",
    "            acc_train = value_pack_train['acc_dircalc']\n",
    "            prec_train = value_pack_train['prec_dircalc']\n",
    "            recall_train = value_pack_train['recall_dircalc']\n",
    "            roc_auc_train = value_pack_train['rocauc_pp_dircalc']\n",
    "\n",
    "            acc_test = value_pack_test['acc_dircalc']\n",
    "            prec_test = value_pack_test['prec_dircalc']\n",
    "            recall_test = value_pack_test['recall_dircalc']\n",
    "            roc_auc_test = value_pack_test['rocauc_pp_dircalc']\n",
    "            \n",
    "            acc_cv_test = value_pack_test['acc_crossval']\n",
    "            prec_cv_test = value_pack_test['prec_crossval']\n",
    "            recall_cv_test = value_pack_test['recall_crossval']\n",
    "            roc_auc_cv_test = value_pack_test['rocauc_crossval']\n",
    "\n",
    "            # ----------------\n",
    "            # Permutation importance of features : probe which features are most predictive\n",
    "            if fea == 0:\n",
    "                feature_names = ['joy', 'joy1derv', 'joy2derv', 'fres', 'freq_t', 'freq_fres']\n",
    "                ovtot = pipeline_permutation_importance(model, X_test, Y_test_1D, feature_names)\n",
    "            else:\n",
    "                ovtot.append('na')\n",
    "            # ----------------\n",
    "\n",
    "            # ----------------\n",
    "            # Save all data to array \n",
    "            res_permod.append([ynum, m_name, fea, desired_col, acc_train, prec_train, recall_train, roc_auc_train, acc_test, prec_test, recall_test, roc_auc_test, acc_cv_test, prec_cv_test, recall_cv_test, roc_auc_cv_test, ovtot])\n",
    "            # ----------------\n",
    "            \n",
    "            # Save data matrices to file per model result :\n",
    "            file_name = \"res_exp_%s_%s_%s_ynum%d_%s_fea%d.pkl\" % (exp, ax_val, ss_val, ynum, m_name, fea)\n",
    "            open_file = open(file_name, \"wb\")\n",
    "            pickle.dump(res_permod, open_file)\n",
    "            open_file.close()\n",
    "            \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215b8ffc",
   "metadata": {},
   "source": [
    "# Batch runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20570d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_main_script(exp, ax_val, ss_val):\n",
    "    df = get_df(ax_val, ss_val)\n",
    "    \n",
    "    feat0, t_feat0, y1_feat0, y2_feat0, y3_feat0 = create_labels_and_initial_feature(df) \n",
    "    \n",
    "    del df\n",
    "    \n",
    "    norm_feat0, norm_feat1, norm_feat2, norm_feat3, feat4, feat4_1 = feature_creation_preprocessing(feat0, t_feat0)\n",
    "    \n",
    "    del feat0, t_feat0\n",
    "    \n",
    "    y_alllabel = [y1_feat0, y2_feat0, y3_feat0]\n",
    "    \n",
    "    # Loop over the different y labels\n",
    "    for ynum in range(len(y_alllabel)):\n",
    "        print('ynum : ', ynum)\n",
    "        \n",
    "        # ----------------\n",
    "        \n",
    "        # Select y\n",
    "        y_label = y_alllabel[ynum]\n",
    "        \n",
    "        # ----------------\n",
    "        \n",
    "        # Put the nested arrays into a DataFrame\n",
    "        # Put the entire trial vector into a single DataFrame\n",
    "\n",
    "        norm_feat0 = np.array(norm_feat0)\n",
    "        # norm_feat0.shape is (442, 471), there are 442 trials\n",
    "        # norm_feat0[0].shape is (471,) means that there are 471 data point in the first trial\n",
    "\n",
    "        df_test = pd.DataFrame()\n",
    "        for tr in range(norm_feat0.shape[0]):\n",
    "\n",
    "            # way 1 : To put a vector into each DataFrame cell  \n",
    "            # data = [norm_feat0[tr]], [norm_feat1[tr]], [norm_feat2[tr]], [norm_feat3[tr]], [feat4[tr]], [feat4_1[tr]], [y_label[tr]]\n",
    "            \n",
    "            data = [norm_feat0[tr]], [norm_feat1[tr]], [norm_feat2[tr]], [norm_feat3[tr]], [y_label[tr]]\n",
    "            \n",
    "            df_row = pd.DataFrame(data=data)\n",
    "            df_row = df_row.T # or df1.transpose()\n",
    "\n",
    "            df_test = pd.concat([df_test, df_row], axis=0)\n",
    "\n",
    "        df_test = df_test.reset_index(drop=True)  # reset index : delete the old index column\n",
    "        \n",
    "        # Automatic label\n",
    "        n_col = df_test.shape[1]\n",
    "        dfl = {}\n",
    "        for nf in range(n_col):\n",
    "            if nf == n_col-1:\n",
    "                dfl[nf] = 'y'\n",
    "            else:\n",
    "                dfl[nf] = '%d' % (nf)\n",
    "        df_test = df_test.rename(dfl, axis=1)\n",
    "        \n",
    "        del data, df_row, y_label\n",
    "        \n",
    "        # ----------------\n",
    "        \n",
    "        df_test2 = pad_data_2makeclasses_equivalent(df_test)\n",
    "        \n",
    "        del df_test\n",
    "        \n",
    "        # ----------------\n",
    "        \n",
    "        # Explode the vectors per DataFrame\n",
    "        featlab_new = explode_without_colnames2(df_test2)\n",
    "        \n",
    "        del df_test2\n",
    "        \n",
    "        # ----------------\n",
    "        \n",
    "        # classification for 3 partitions of feature data per 8 classifiers\n",
    "        classify(featlab_new, ynum)\n",
    "        \n",
    "        del featlab_new\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658a67e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "exp = 'rot'\n",
    "ax_val = 'all'\n",
    "ss_val = 'all'\n",
    "run_main_script(exp, ax_val, ss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a413f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Manual runs\n",
    "# Justification for not doing a loop : Can not do a loop because the computer stops or it stops \n",
    "# for weird reasons.  Have to run each manually.\n",
    "# ------------------------------\n",
    "\n",
    "\n",
    "# -------------\n",
    "# DONE\n",
    "# -------------\n",
    "\n",
    "exp = 'rot'\n",
    "ax_val = 'all'\n",
    "ss_val = 'all'\n",
    "run_main_script(exp, ax_val, ss_val)\n",
    "\n",
    "exp = 'rot'\n",
    "ax_val = 'all'\n",
    "ss_val = 'sub'\n",
    "run_main_script(exp, ax_val, ss_val)\n",
    "\n",
    "exp = 'rot'\n",
    "ax_val = 'all'\n",
    "ss_val = 'sup'\n",
    "run_main_script(exp, ax_val, ss_val)\n",
    "\n",
    "# -------------\n",
    "\n",
    "exp = 'trans'\n",
    "ax_val = 'all'\n",
    "ss_val = 'all'\n",
    "run_main_script(exp, ax_val, ss_val)\n",
    "\n",
    "exp = 'trans'\n",
    "ax_val = 'all'\n",
    "ss_val = 'sub'\n",
    "run_main_script(exp, ax_val, ss_val)\n",
    "\n",
    "exp = 'trans'\n",
    "ax_val = 'all'\n",
    "ss_val = 'sup'\n",
    "run_main_script(exp, ax_val, ss_val)\n",
    "\n",
    "# -------------\n",
    "\n",
    "exp = 'rot'\n",
    "ax_val = 'ax0'\n",
    "ss_val = 'all'\n",
    "run_main_script(exp, ax_val, ss_val)\n",
    "\n",
    "exp = 'rot'\n",
    "ax_val = 'ax1'\n",
    "ss_val = 'all'\n",
    "run_main_script(exp, ax_val, ss_val)\n",
    "\n",
    "exp = 'rot'\n",
    "ax_val = 'ax2'\n",
    "ss_val = 'all'\n",
    "run_main_script(exp, ax_val, ss_val)\n",
    "\n",
    "# -------------\n",
    "\n",
    "exp = 'trans'\n",
    "ax_val = 'ax0'\n",
    "ss_val = 'all'\n",
    "run_main_script(exp, ax_val, ss_val)\n",
    "\n",
    "exp = 'trans'\n",
    "ax_val = 'ax1'\n",
    "ss_val = 'all'\n",
    "run_main_script(exp, ax_val, ss_val)\n",
    "\n",
    "exp = 'trans'\n",
    "ax_val = 'ax2'\n",
    "ss_val = 'all'\n",
    "run_main_script(exp, ax_val, ss_val)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
